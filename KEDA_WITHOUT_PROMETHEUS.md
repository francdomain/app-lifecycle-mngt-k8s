# KEDA Without Prometheus - Configuration Guide

## Overview

Your KEDA setup has been updated to work **without Prometheus** by using built-in Kubernetes CPU and Memory metrics. This provides immediate event-driven autoscaling without requiring additional infrastructure.

## Current Configuration

All ScaledObjects now use **CPU and Memory triggers** as primary metrics:

### Frontend Service

- **Primary**: CPU 70%, Memory 80%
- **Range**: 2-10 replicas
- **Behavior**: Aggressive (100% scale-up per 30s, 50% scale-down per 60s)

### API Gateway

- **Primary**: CPU 75%, Memory 80%
- **Range**: 2-8 replicas
- **Behavior**: Aggressive scaling both directions

### Product Service

- **Primary**: CPU 80%, Memory 75%
- **Range**: 2-6 replicas
- **Behavior**: Moderate (50% scale-up per 60s, 25% scale-down per 120s)

### Order Service

- **Primary**: CPU 80%, Memory 80%
- **Range**: 2-6 replicas
- **Behavior**: Aggressive (100% scale-up per 30s, 50% scale-down per 60s)

## How It Works

```
Kubernetes Metrics Server (built-in)
    ↓ (provides CPU/Memory metrics)
KEDA ScaledObjects
    ├─ Evaluate CPU/Memory utilization
    ├─ Calculate desired replica count
    └─ Update HPA
       ↓
Kubernetes HPA (auto-generated by KEDA)
    ├─ Compare current vs desired replicas
    └─ Scale deployment
       ↓
Deployments
    └─ Pods created/terminated
```

## Deployment

### 1. Deploy KEDA

```bash
./scripts/deploy-keda.sh ecommerce
```

### 2. Verify Installation

```bash
# Check KEDA operator
kubectl get pods -n keda
kubectl get deployment keda-operator -n keda

# Check ScaledObjects
kubectl get scaledobjects -n ecommerce
kubectl describe scaledobject frontend-scaler -n ecommerce

# Check generated HPAs
kubectl get hpa -n ecommerce
kubectl describe hpa keda-frontend-scaler -n ecommerce
```

### 3. Test Scaling

```bash
# Monitor pods scaling
kubectl get pods -n ecommerce --watch

# View replica counts
kubectl get deployments -n ecommerce --watch

# Generate CPU load to trigger scaling
kubectl run -n ecommerce load-test --image=busybox -it --rm -- \
  /bin/sh -c "while true; do echo 'scale=10000; sqrt(2)' | bc; done"

# Watch scaling happen
kubectl get hpa -n ecommerce --watch
```

## Monitoring

### View Current Status

```bash
# ScaledObjects status
kubectl get scaledobjects -n ecommerce -o wide

# Generated HPA status
kubectl get hpa -n ecommerce -o wide

# Detailed HPA metrics
kubectl top pod -n ecommerce
kubectl top nodes
```

### Monitor Scaling Events

```bash
# Real-time monitoring
kubectl get pods,hpa -n ecommerce --watch

# View events
kubectl get events -n ecommerce --sort-by='.lastTimestamp' | tail -20

# Check HPA conditions
kubectl describe hpa keda-frontend-scaler -n ecommerce | grep -A 5 "Conditions:"
```

## Adding Prometheus Later (Optional)

If you want to add custom HTTP request-based metrics, you can add Prometheus support later without changing the KEDA configuration structure.

### Step 1: Install Prometheus

```bash
# Using Helm (recommended)
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace

# Or use your existing Prometheus deployment
```

### Step 2: Update ScaledObjects

Once Prometheus is running, update `manifests/07-keda-scalers.yaml` to add Prometheus triggers:

```yaml
---
# Frontend ScaledObject - with optional Prometheus metrics
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: frontend-scaler
  namespace: ecommerce
spec:
  scaleTargetRef:
    name: frontend
  minReplicaCount: 2
  maxReplicaCount: 10
  triggers:
    # Optional: HTTP request rate trigger (requires Prometheus)
    - type: prometheus
      metadata:
        serverAddress: http://prometheus:9090
        metricName: http_request_rate
        threshold: "100"
        query: |
          sum(rate(nginx_http_requests_total{job="frontend"}[30s]))
    # Primary: CPU-based trigger (always available)
    - type: cpu
      metricType: Utilization
      metadata:
        type: Utilization
        value: "70"
    # Secondary: Memory trigger (always available)
    - type: memory
      metricType: Utilization
      metadata:
        type: Utilization
        value: "80"
```

### Step 3: Apply Changes

```bash
# Update ScaledObjects with Prometheus triggers
kubectl apply -f manifests/07-keda-scalers.yaml

# Verify Prometheus metrics are being collected
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Visit http://localhost:9090 in browser
```

## Troubleshooting

### ScaledObject Not Scaling

**Check ScaledObject Status**:

```bash
kubectl describe scaledobject frontend-scaler -n ecommerce
```

**Verify Metrics Available**:

```bash
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes | jq .
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/ecommerce/pods | jq .
```

**Check KEDA Operator Logs**:

```bash
kubectl logs -n keda -l app=keda -f
```

### Metrics Server Not Responding

**Check Metrics Server**:

```bash
kubectl get deployment metrics-server -n kube-system
kubectl logs -n kube-system -l k8s-app=metrics-server
```

**If Metrics Server Missing**:

```bash
# Install Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### HPA Not Created

**Verify ScaledObject Syntax**:

```bash
kubectl apply -f manifests/07-keda-scalers.yaml --dry-run=client -o yaml
```

**Check KEDA Operator Logs**:

```bash
kubectl logs -n keda -l app=keda | grep -i "error\|hpa"
```

## Performance Characteristics

### Scaling Latency

- Metric collection: ~15 seconds
- KEDA evaluation: ~30 seconds (default interval)
- HPA decision: <1 second
- Pod startup: 5-15 seconds
- **Total scale-out time**: 50-75 seconds

### Resource Overhead

- KEDA operator: ~50-100MB memory
- Metrics server: Built-in (no additional overhead)
- Total overhead: ~100MB

### Cost

- KEDA: Minimal (same as any other control plane component)
- No external metric storage needed
- No additional services required

## Configuration Adjustment

### Increase Responsiveness (Scale Faster)

To scale up faster, reduce stabilization window:

```yaml
triggers:
  - type: cpu
    metricType: Utilization
    metadata:
      type: Utilization
      value: "50" # Lower threshold = scale sooner

advanced:
  horizontalPodAutoscalerConfig:
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 0 # Scale immediately
        policies:
          - type: Percent
            value: 100
            periodSeconds: 15 # Check every 15 seconds
```

### Reduce Cost (Scale More Conservatively)

```yaml
advanced:
  horizontalPodAutoscalerConfig:
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 600 # Wait 10 minutes
        policies:
          - type: Percent
            value: 25 # Reduce by 25%
            periodSeconds: 180 # Every 3 minutes
```

### Balance Performance and Cost

```yaml
advanced:
  horizontalPodAutoscalerConfig:
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 30
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
```

## Migration Path

### From HPA to KEDA

Your original HPA configuration (`manifests/05-hpa.yaml`) still exists. To fully migrate to KEDA:

```bash
# Keep both running for comparison
kubectl get hpa,scaledobjects -n ecommerce -w

# When satisfied with KEDA behavior
kubectl delete hpa -n ecommerce -l app=ecommerce

# KEDA's generated HPAs take over
kubectl get hpa -n ecommerce
```

### Rollback to HPA Only

If you need to revert to HPA:

```bash
# Restore HPA
kubectl apply -f manifests/05-hpa.yaml

# Pause KEDA (HPA continues)
kubectl patch scaledobject -all -p '{"spec":{"paused":true}}'

# Or delete ScaledObjects entirely
kubectl delete scaledobjects -n ecommerce --all
```

## Advantages of This Setup

✅ **Works Immediately** - No external dependencies
✅ **Built-in Metrics** - Kubernetes metrics server is included
✅ **Low Overhead** - Minimal CPU and memory usage
✅ **Simple Configuration** - CPU and memory are straightforward
✅ **Production Ready** - Proven and reliable
✅ **Scalable** - Works with any Kubernetes cluster

## When to Add Prometheus

Consider adding Prometheus when you need:

- **Request-based scaling** - Scale based on HTTP requests/sec
- **Latency-based scaling** - Scale when response time increases
- **Business metrics** - Scale based on application-specific metrics
- **Queue depth scaling** - Scale based on message queue size
- **Advanced analytics** - Detailed observability and dashboards

Until then, CPU/Memory-based scaling provides solid automatic scaling capabilities.

## References

- [Kubernetes Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
- [KEDA Documentation](https://keda.sh)
- [HPA Documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

---

**Status**: ✅ Configured for CPU/Memory-based autoscaling
**Prometheus**: Optional (can be added later)
**Ready for Production**: Yes
